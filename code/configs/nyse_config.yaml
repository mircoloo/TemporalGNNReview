# -----------------------------------------------------------------
# Configuration for the NASDAQ market (for the NEW DGDNN model)
# -----------------------------------------------------------------

# Parameters related to data loading and time periods
dataset_params:
  market: 'NYSE'
  train_sedate: ['2016-05-01', '2017-06-30']
  val_sedate: ['2017-07-01', '2017-12-31']
  test_sedate: ['2018-01-01', '2019-12-31']
  window_size: 22  # From paper: τ = 19
  use_fast_approximation: false

# -----------------------------------------------------------------

# Parameters defining the NEW DGDNN model architecture
model_params:
  layers: 9 
  expansion_step: 10  # From paper: K = 9
  num_heads: 3     # From paper should be 3
  active_layers: [true, true, true, true, false, false, false, false, true]

  # --- New simplified attention parameters ---
  # The output size for ALL attention layers. From paper's "embedding dimension".
  embedding_output_size: 128
  # The size of the internal MLP inside the attention block. 256 is a sensible default.
  embedding_hidden_size: 256
  # The size the raw features are projected to. Must be divisible by num_heads (3). 96 is a good choice.
  raw_feature_size: 96

  # --- Calculated lists based on the new rules ---
  # Length must be layers + 1 = 9. First element must be 5 * window_size = 95.
  diffusion_size: [110, 128, 256, 512, 256, 128, 128, 128, 128, 128]

  # Length must be layers = 8. Values are calculated input dimensions for each attention layer.
  embedding_size:
    - 224  # Layer 0: diffusion_size[1] + raw_feature_size = 128 + 96
    - 384  # Layer 1: diffusion_size[2] + embedding_output_size = 256 + 128
    - 640  # Layer 2: diffusion_size[3] + embedding_output_size = 512 + 128
    - 384  # Layer 3: diffusion_size[4] + embedding_output_size = 256 + 128
    - 256  # Layer 4: diffusion_size[5] + embedding_output_size = 128 + 128
    - 256  # Layer 5: diffusion_size[6] + embedding_output_size = 128 + 128
    - 256  # Layer 6: diffusion_size[7] + embedding_output_size = 128 + 128
    - 256  # Layer 7: diffusion_size[8] + embedding_output_size = 128 + 128
    - 256  # Layer 8: diffusion_size[9] + embedding_output_size = 128 + 128

# -----------------------------------------------------------------

# Parameters for the training process
training_params:
  learning_rate: 0.0002      # From paper: 2e-4
  weight_decay: 0.000015     # From paper: 1.5e-5
  epochs: 1200               # From paper
  neighbour_radius_coeff: 2.7e-3  # From paper: α = 2.9e-3